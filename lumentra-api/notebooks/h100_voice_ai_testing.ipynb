{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lumentra Voice AI - Groq Native Tool Calling Testing\n",
    "\n",
    "Test Groq LLM with native tool calling for voice AI applications.\n",
    "\n",
    "**Current Production Stack:**\n",
    "- SignalWire: Telephony (SIP/WebRTC)\n",
    "- Deepgram: Speech-to-Text (Nova-2)\n",
    "- Groq: LLM with Native Tool Calling\n",
    "- Cartesia: Text-to-Speech (Sonic)\n",
    "\n",
    "**Available Groq Models:**\n",
    "1. `llama-3.1-8b-instant` - Fast, balanced (default)\n",
    "2. `gpt-oss-20b-128k` - Extended context\n",
    "3. `qwen3-32b-131k` - Complex reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install Groq SDK\n",
    "!pip install -q groq python-dotenv\n",
    "\n",
    "import os\n",
    "from groq import Groq\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Set your Groq API key\n",
    "# Option 1: Use Colab secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "    print(\"Using GROQ_API_KEY from Colab secrets\")\n",
    "except:\n",
    "    # Option 2: Set directly\n",
    "    GROQ_API_KEY = os.environ.get('GROQ_API_KEY', 'YOUR_KEY_HERE')\n",
    "    print(\"Using GROQ_API_KEY from environment\")\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "print(\"Groq client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tool Definitions\n",
    "\n",
    "Define tools for the voice agent using Groq's native tool calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Voice Agent Tool Definitions\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"check_availability\",\n",
    "            \"description\": \"Check available appointment slots for a specific date\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Date in YYYY-MM-DD format\"\n",
    "                    },\n",
    "                    \"service_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Type of service (optional)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"date\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"create_booking\",\n",
    "            \"description\": \"Create a new booking for the customer\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"customer_name\": {\"type\": \"string\", \"description\": \"Customer's full name\"},\n",
    "                    \"customer_phone\": {\"type\": \"string\", \"description\": \"Customer's phone number\"},\n",
    "                    \"date\": {\"type\": \"string\", \"description\": \"Booking date (YYYY-MM-DD)\"},\n",
    "                    \"time\": {\"type\": \"string\", \"description\": \"Booking time (HH:MM)\"},\n",
    "                    \"service_type\": {\"type\": \"string\", \"description\": \"Type of service\"}\n",
    "                },\n",
    "                \"required\": [\"customer_name\", \"customer_phone\", \"date\", \"time\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_contact\",\n",
    "            \"description\": \"Look up a contact by phone number or name\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"phone\": {\"type\": \"string\", \"description\": \"Phone number to search\"},\n",
    "                    \"name\": {\"type\": \"string\", \"description\": \"Name to search\"}\n",
    "                },\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"transfer_to_human\",\n",
    "            \"description\": \"Transfer the call to a human staff member\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"reason\": {\"type\": \"string\", \"description\": \"Reason for transfer\"}\n",
    "                },\n",
    "                \"required\": [\"reason\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(TOOLS)} tools for voice agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Groq Chat with Native Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are Luna, the AI voice assistant for Stellar Auto Service.\n",
    "\n",
    "You help callers with:\n",
    "- Booking service appointments\n",
    "- Checking availability\n",
    "- Looking up customer records\n",
    "- General inquiries about services\n",
    "\n",
    "Keep responses concise and natural for voice conversation.\n",
    "Use tools when appropriate to fulfill customer requests.\"\"\"\n",
    "\n",
    "def chat_with_groq(\n",
    "    user_message: str,\n",
    "    history: list = None,\n",
    "    model: str = \"llama-3.1-8b-instant\"\n",
    ") -> dict:\n",
    "    \"\"\"Chat with Groq using native tool calling.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    \n",
    "    if history:\n",
    "        messages.extend(history)\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=TOOLS,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    latency_ms = (time.time() - start) * 1000\n",
    "    choice = response.choices[0]\n",
    "    \n",
    "    result = {\n",
    "        \"model\": model,\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if choice.message.tool_calls:\n",
    "        tool_call = choice.message.tool_calls[0]\n",
    "        result[\"action\"] = \"tool_call\"\n",
    "        result[\"tool\"] = tool_call.function.name\n",
    "        result[\"arguments\"] = json.loads(tool_call.function.arguments)\n",
    "        result[\"text\"] = None\n",
    "    else:\n",
    "        result[\"action\"] = \"response\"\n",
    "        result[\"tool\"] = None\n",
    "        result[\"arguments\"] = None\n",
    "        result[\"text\"] = choice.message.content\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"chat_with_groq function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test messages that should trigger different behaviors\n",
    "test_messages = [\n",
    "    \"Hi there!\",  # Should get conversational response\n",
    "    \"What times are available tomorrow?\",  # Should call check_availability\n",
    "    \"I'd like to book an oil change for tomorrow at 2pm, my name is John Smith\",  # Should call create_booking\n",
    "    \"Can you look up my account? My number is 555-123-4567\",  # Should call get_contact\n",
    "    \"I need to speak to a manager about a complaint\",  # Should call transfer_to_human\n",
    "]\n",
    "\n",
    "print(\"Testing Groq Native Tool Calling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for msg in test_messages:\n",
    "    result = chat_with_groq(msg)\n",
    "    print(f\"\\nInput: {msg}\")\n",
    "    print(f\"Action: {result['action']}\")\n",
    "    if result['tool']:\n",
    "        print(f\"Tool: {result['tool']}\")\n",
    "        print(f\"Args: {json.dumps(result['arguments'], indent=2)}\")\n",
    "    else:\n",
    "        print(f\"Response: {result['text'][:100]}...\" if len(result.get('text', '') or '') > 100 else f\"Response: {result.get('text')}\")\n",
    "    print(f\"Latency: {result['latency_ms']:.1f}ms\")\n",
    "    print(f\"Tokens: {result['usage']['prompt_tokens']}+{result['usage']['completion_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare different Groq models\n",
    "MODELS = [\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    # \"gpt-oss-20b-128k\",  # Uncomment if available\n",
    "    # \"qwen3-32b-131k\",    # Uncomment if available\n",
    "]\n",
    "\n",
    "test_msg = \"I want to book an appointment for tomorrow at 3pm for brake service. My name is Sarah.\"\n",
    "\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in MODELS:\n",
    "    try:\n",
    "        result = chat_with_groq(test_msg, model=model)\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"Action: {result['action']}\")\n",
    "        if result['tool']:\n",
    "            print(f\"Tool: {result['tool']}\")\n",
    "            print(f\"Args: {result['arguments']}\")\n",
    "        print(f\"Latency: {result['latency_ms']:.1f}ms\")\n",
    "        print(f\"Tokens: {result['usage']['prompt_tokens']}+{result['usage']['completion_tokens']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Pipeline with Tool Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def execute_tool(tool_name: str, arguments: dict) -> dict:\n",
    "    \"\"\"Simulate tool execution (in production, these call actual APIs).\"\"\"\n",
    "    if tool_name == \"check_availability\":\n",
    "        return {\n",
    "            \"available\": True,\n",
    "            \"slots\": [\"9:00 AM\", \"11:00 AM\", \"2:00 PM\", \"4:00 PM\"],\n",
    "            \"date\": arguments.get(\"date\", \"tomorrow\")\n",
    "        }\n",
    "    elif tool_name == \"create_booking\":\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"confirmation_code\": \"SA-2024-12345\",\n",
    "            \"details\": arguments\n",
    "        }\n",
    "    elif tool_name == \"get_contact\":\n",
    "        return {\n",
    "            \"found\": True,\n",
    "            \"name\": \"John Smith\",\n",
    "            \"tier\": \"VIP\",\n",
    "            \"last_visit\": \"2024-01-15\"\n",
    "        }\n",
    "    elif tool_name == \"transfer_to_human\":\n",
    "        return {\n",
    "            \"transferred\": True,\n",
    "            \"department\": \"Customer Service\"\n",
    "        }\n",
    "    return {\"error\": \"Unknown tool\"}\n",
    "\n",
    "def voice_pipeline(user_message: str, history: list = None) -> dict:\n",
    "    \"\"\"Complete voice AI pipeline with Groq native tool calling.\"\"\"\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # Step 1: Get LLM response with potential tool call\n",
    "    llm_result = chat_with_groq(user_message, history)\n",
    "    \n",
    "    if llm_result[\"action\"] == \"tool_call\":\n",
    "        # Step 2: Execute the tool\n",
    "        tool_result = execute_tool(llm_result[\"tool\"], llm_result[\"arguments\"])\n",
    "        \n",
    "        # Step 3: Get final response with tool result\n",
    "        tool_response_msg = f\"Tool {llm_result['tool']} returned: {json.dumps(tool_result)}\"\n",
    "        \n",
    "        final_history = (history or []) + [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": f\"[Calling {llm_result['tool']}]\"},\n",
    "            {\"role\": \"user\", \"content\": tool_response_msg}\n",
    "        ]\n",
    "        \n",
    "        final_result = chat_with_groq(\n",
    "            \"Please provide a natural response to the customer based on the tool result.\",\n",
    "            final_history\n",
    "        )\n",
    "        \n",
    "        total_latency = (time.time() - total_start) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"text\": final_result.get(\"text\", \"Your request has been processed.\"),\n",
    "            \"tool_called\": llm_result[\"tool\"],\n",
    "            \"tool_args\": llm_result[\"arguments\"],\n",
    "            \"tool_result\": tool_result,\n",
    "            \"llm_latency_ms\": llm_result[\"latency_ms\"],\n",
    "            \"total_latency_ms\": total_latency\n",
    "        }\n",
    "    else:\n",
    "        total_latency = (time.time() - total_start) * 1000\n",
    "        return {\n",
    "            \"text\": llm_result[\"text\"],\n",
    "            \"tool_called\": None,\n",
    "            \"tool_args\": None,\n",
    "            \"tool_result\": None,\n",
    "            \"llm_latency_ms\": llm_result[\"latency_ms\"],\n",
    "            \"total_latency_ms\": total_latency\n",
    "        }\n",
    "\n",
    "# Test complete pipeline\n",
    "print(\"Complete Voice Pipeline Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline_tests = [\n",
    "    \"Hello, I need some help\",\n",
    "    \"Do you have anything available tomorrow morning?\",\n",
    "    \"Great, book me for 11am. Name is Mike Jones, number 555-9876\",\n",
    "    \"Can you transfer me to someone about my warranty?\"\n",
    "]\n",
    "\n",
    "history = []\n",
    "for msg in pipeline_tests:\n",
    "    result = voice_pipeline(msg, history)\n",
    "    print(f\"\\nUser: {msg}\")\n",
    "    print(f\"Luna: {result['text']}\")\n",
    "    if result['tool_called']:\n",
    "        print(f\"[Tool: {result['tool_called']} -> {result['tool_result']}]\")\n",
    "    print(f\"Latency: {result['total_latency_ms']:.1f}ms\")\n",
    "    \n",
    "    # Update history\n",
    "    history.append({\"role\": \"user\", \"content\": msg})\n",
    "    history.append({\"role\": \"assistant\", \"content\": result['text']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Latency Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "benchmark_messages = [\n",
    "    \"Hi\",\n",
    "    \"Hello there\",\n",
    "    \"I want to book an appointment\",\n",
    "    \"What times are available?\",\n",
    "    \"Do you have anything tomorrow?\",\n",
    "    \"Can I book for 2pm?\",\n",
    "    \"What are your hours?\",\n",
    "    \"How much does an oil change cost?\",\n",
    "    \"I need to reschedule\",\n",
    "    \"Thanks, bye!\",\n",
    "] * 5  # 50 messages\n",
    "\n",
    "random.shuffle(benchmark_messages)\n",
    "\n",
    "print(f\"Running benchmark with {len(benchmark_messages)} messages...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "latencies = []\n",
    "tool_calls = 0\n",
    "\n",
    "for i, msg in enumerate(benchmark_messages):\n",
    "    result = chat_with_groq(msg)\n",
    "    latencies.append(result['latency_ms'])\n",
    "    if result['tool']:\n",
    "        tool_calls += 1\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(benchmark_messages)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BENCHMARK RESULTS (Groq API - llama-3.1-8b-instant)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total messages: {len(benchmark_messages)}\")\n",
    "print(f\"Tool calls: {tool_calls} ({100*tool_calls/len(benchmark_messages):.1f}%)\")\n",
    "print(f\"\\nLatency Statistics:\")\n",
    "print(f\"  Mean: {sum(latencies)/len(latencies):.1f}ms\")\n",
    "print(f\"  Min: {min(latencies):.1f}ms\")\n",
    "print(f\"  Max: {max(latencies):.1f}ms\")\n",
    "print(f\"  P50: {sorted(latencies)[len(latencies)//2]:.1f}ms\")\n",
    "print(f\"  P95: {sorted(latencies)[int(len(latencies)*0.95)]:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Production Voice AI Stack:**\n",
    "\n",
    "| Component | Service | Latency |\n",
    "|-----------|---------|--------|\n",
    "| Telephony | SignalWire | ~50ms |\n",
    "| STT | Deepgram Nova-2 | ~100ms |\n",
    "| LLM | Groq (llama-3.1-8b) | ~200ms |\n",
    "| TTS | Cartesia Sonic | ~100ms |\n",
    "\n",
    "**Total End-to-End: ~450ms**\n",
    "\n",
    "```\n",
    "User Speech -> SignalWire -> Deepgram STT -> Groq LLM (Native Tools)\n",
    "                                                  |\n",
    "                                    +-------------+-------------+\n",
    "                                    |             |             |\n",
    "                              Direct Response  Tool Call    Transfer\n",
    "                                    |             |             |\n",
    "                                    +------+------+             |\n",
    "                                           |                    |\n",
    "                                    Cartesia TTS <- - - - - - - +\n",
    "                                           |\n",
    "                                    SignalWire -> User\n",
    "```\n",
    "\n",
    "**Advantages over FunctionGemma approach:**\n",
    "- No local GPU required\n",
    "- Single LLM call for routing + response\n",
    "- Better context understanding\n",
    "- Simpler architecture\n",
    "- Native tool calling support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
