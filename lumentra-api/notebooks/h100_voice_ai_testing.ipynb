{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lumentra Voice AI - H100 Testing Notebook\n",
        "\n",
        "Test FunctionGemma 270M (router) + Gemma 3 12B / Qwen 32B (main LLM) on H100.\n",
        "\n",
        "**Runtime Requirements:**\n",
        "- GPU: A100/H100 (40GB+ VRAM)\n",
        "- High-RAM runtime recommended for Qwen 32B\n",
        "\n",
        "**Models Tested:**\n",
        "1. FunctionGemma 270M - Fast function calling router (~550MB)\n",
        "2. Gemma 3 12B - Main conversation + tool calling\n",
        "3. Qwen3 32B - Heavy backup for complex queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q vllm  # For fast inference\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. FunctionGemma 270M Setup\n",
        "\n",
        "Ultra-fast function calling router. Runs on CPU or GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "\n",
        "# Load FunctionGemma 270M\n",
        "print(\"Loading FunctionGemma 270M...\")\n",
        "fg_model_id = \"google/functiongemma-2b\"  # Using 2B version; 270M not on HF yet\n",
        "\n",
        "fg_tokenizer = AutoTokenizer.from_pretrained(fg_model_id)\n",
        "fg_model = AutoModelForCausalLM.from_pretrained(\n",
        "    fg_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(f\"FunctionGemma loaded on {fg_model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tool definitions for FunctionGemma\n",
        "TOOLS = [\n",
        "    {\n",
        "        \"name\": \"check_availability\",\n",
        "        \"description\": \"Check available appointment slots for a specific date\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"date\": {\"type\": \"string\", \"description\": \"Date in YYYY-MM-DD format\"},\n",
        "                \"service_type\": {\"type\": \"string\", \"description\": \"Type of service\"}\n",
        "            },\n",
        "            \"required\": [\"date\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"create_booking\",\n",
        "        \"description\": \"Create a new booking for the customer\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"customer_name\": {\"type\": \"string\"},\n",
        "                \"customer_phone\": {\"type\": \"string\"},\n",
        "                \"date\": {\"type\": \"string\"},\n",
        "                \"time\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"customer_name\", \"customer_phone\", \"date\", \"time\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"transfer_to_human\",\n",
        "        \"description\": \"Transfer call to human staff\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"reason\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"reason\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "import json\n",
        "\n",
        "def build_fg_prompt(user_message: str) -> str:\n",
        "    \"\"\"Build FunctionGemma prompt.\"\"\"\n",
        "    tools_json = json.dumps(TOOLS, indent=2)\n",
        "    return f\"\"\"You are a function calling AI. Given the user message and available tools, decide if a function should be called.\n",
        "\n",
        "Available tools:\n",
        "{tools_json}\n",
        "\n",
        "User message: {user_message}\n",
        "\n",
        "If a function should be called, respond with:\n",
        "<start_function_call>function_name(param1=\"value1\", param2=\"value2\")<end_function_call>\n",
        "\n",
        "If no function is needed, respond with:\n",
        "<no_function_call>\n",
        "\n",
        "Response:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_with_functiongemma(user_message: str) -> dict:\n",
        "    \"\"\"Route user message using FunctionGemma.\"\"\"\n",
        "    start = time.time()\n",
        "    \n",
        "    prompt = build_fg_prompt(user_message)\n",
        "    inputs = fg_tokenizer(prompt, return_tensors=\"pt\").to(fg_model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = fg_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=fg_tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = fg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response[len(prompt):].strip()\n",
        "    \n",
        "    latency_ms = (time.time() - start) * 1000\n",
        "    \n",
        "    # Parse response\n",
        "    import re\n",
        "    func_match = re.search(r'<start_function_call>(.+?)<end_function_call>', response)\n",
        "    \n",
        "    if func_match:\n",
        "        func_call = func_match.group(1)\n",
        "        # Parse function name and args\n",
        "        name_match = re.match(r'(\\w+)\\((.*)\\)', func_call)\n",
        "        if name_match:\n",
        "            func_name = name_match.group(1)\n",
        "            args_str = name_match.group(2)\n",
        "            # Simple arg parsing\n",
        "            args = {}\n",
        "            for arg_match in re.finditer(r'(\\w+)=\"([^\"]+)\"', args_str):\n",
        "                args[arg_match.group(1)] = arg_match.group(2)\n",
        "            \n",
        "            return {\n",
        "                \"action\": \"function_call\",\n",
        "                \"function\": func_name,\n",
        "                \"arguments\": args,\n",
        "                \"latency_ms\": latency_ms,\n",
        "                \"raw_output\": response\n",
        "            }\n",
        "    \n",
        "    return {\n",
        "        \"action\": \"llm_required\",\n",
        "        \"latency_ms\": latency_ms,\n",
        "        \"raw_output\": response\n",
        "    }\n",
        "\n",
        "# Test FunctionGemma routing\n",
        "test_messages = [\n",
        "    \"Hi there!\",\n",
        "    \"I want to book an appointment for tomorrow at 2pm\",\n",
        "    \"What times are available on Friday?\",\n",
        "    \"I need to speak to a manager about a refund\",\n",
        "    \"Can you check availability for next Monday?\",\n",
        "]\n",
        "\n",
        "print(\"Testing FunctionGemma routing:\")\n",
        "print(\"=\" * 60)\n",
        "for msg in test_messages:\n",
        "    result = route_with_functiongemma(msg)\n",
        "    print(f\"\\nInput: {msg}\")\n",
        "    print(f\"Action: {result['action']}\")\n",
        "    if result['action'] == 'function_call':\n",
        "        print(f\"Function: {result['function']}\")\n",
        "        print(f\"Args: {result['arguments']}\")\n",
        "    print(f\"Latency: {result['latency_ms']:.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Gemma 3 12B Setup\n",
        "\n",
        "Main conversation model with tool calling capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Gemma 3 12B (or Gemma 2 9B if 3 not available)\n",
        "print(\"Loading Gemma model...\")\n",
        "\n",
        "gemma_model_id = \"google/gemma-2-9b-it\"  # Using Gemma 2 9B IT\n",
        "\n",
        "gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_model_id)\n",
        "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gemma_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    # Use 4-bit quantization to fit in memory\n",
        "    # load_in_4bit=True,  # Uncomment if memory constrained\n",
        ")\n",
        "print(f\"Gemma loaded on {gemma_model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are Luna, the AI voice assistant for Sunrise Salon.\n",
        "\n",
        "You help callers with booking appointments, checking availability, and general inquiries.\n",
        "Keep responses concise and natural for voice conversation.\n",
        "\n",
        "Available tools:\n",
        "- check_availability(date): Check available time slots\n",
        "- create_booking(customer_name, customer_phone, date, time): Book an appointment\n",
        "- transfer_to_human(reason): Transfer to staff\n",
        "\n",
        "When using tools, format as: <tool>tool_name(args)</tool>\n",
        "After tool use, provide a natural response based on the result.\"\"\"\n",
        "\n",
        "def chat_with_gemma(user_message: str, history: list = None) -> dict:\n",
        "    \"\"\"Chat with Gemma model.\"\"\"\n",
        "    start = time.time()\n",
        "    \n",
        "    if history is None:\n",
        "        history = []\n",
        "    \n",
        "    # Build conversation\n",
        "    messages = [{\"role\": \"user\", \"content\": SYSTEM_PROMPT}]\n",
        "    messages.append({\"role\": \"assistant\", \"content\": \"I understand. I'm Luna, ready to help customers.\"})\n",
        "    \n",
        "    for h in history:\n",
        "        messages.append(h)\n",
        "    \n",
        "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "    \n",
        "    # Format for Gemma\n",
        "    prompt = gemma_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    inputs = gemma_tokenizer(prompt, return_tensors=\"pt\").to(gemma_model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = gemma_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=gemma_tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = gemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the assistant response\n",
        "    response = response.split(\"<start_of_turn>model\\n\")[-1]\n",
        "    response = response.split(\"<end_of_turn>\")[0].strip()\n",
        "    \n",
        "    latency_ms = (time.time() - start) * 1000\n",
        "    \n",
        "    return {\n",
        "        \"text\": response,\n",
        "        \"latency_ms\": latency_ms\n",
        "    }\n",
        "\n",
        "# Test Gemma conversation\n",
        "print(\"Testing Gemma conversation:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_convos = [\n",
        "    \"Hi, I'd like to book a haircut\",\n",
        "    \"Do you have anything available tomorrow afternoon?\",\n",
        "    \"2pm works for me. My name is John and my number is 555-1234\",\n",
        "]\n",
        "\n",
        "history = []\n",
        "for msg in test_convos:\n",
        "    result = chat_with_gemma(msg, history)\n",
        "    print(f\"\\nUser: {msg}\")\n",
        "    print(f\"Luna: {result['text']}\")\n",
        "    print(f\"Latency: {result['latency_ms']:.1f}ms\")\n",
        "    history.append({\"role\": \"user\", \"content\": msg})\n",
        "    history.append({\"role\": \"assistant\", \"content\": result['text']})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Complete Pipeline Test\n",
        "\n",
        "FunctionGemma routes -> Gemma/Qwen handles conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def voice_pipeline(user_message: str, history: list = None) -> dict:\n",
        "    \"\"\"Complete voice AI pipeline.\"\"\"\n",
        "    total_start = time.time()\n",
        "    \n",
        "    # Step 1: Route with FunctionGemma\n",
        "    route_result = route_with_functiongemma(user_message)\n",
        "    \n",
        "    if route_result[\"action\"] == \"function_call\":\n",
        "        # Direct function execution\n",
        "        func_name = route_result[\"function\"]\n",
        "        func_args = route_result[\"arguments\"]\n",
        "        \n",
        "        # Simulate tool execution\n",
        "        if func_name == \"check_availability\":\n",
        "            tool_result = {\"available\": True, \"slots\": [\"10am\", \"2pm\", \"4pm\"]}\n",
        "            response = f\"I have availability at 10am, 2pm, and 4pm. Which time works best for you?\"\n",
        "        elif func_name == \"create_booking\":\n",
        "            tool_result = {\"success\": True, \"confirmation\": \"BK12345\"}\n",
        "            response = f\"Your appointment is booked. Your confirmation code is BK12345.\"\n",
        "        elif func_name == \"transfer_to_human\":\n",
        "            tool_result = {\"transferred\": True}\n",
        "            response = \"I'll transfer you to our team now. Please hold.\"\n",
        "        else:\n",
        "            tool_result = {}\n",
        "            response = \"I'll help you with that.\"\n",
        "        \n",
        "        total_latency = (time.time() - total_start) * 1000\n",
        "        return {\n",
        "            \"text\": response,\n",
        "            \"tool_called\": func_name,\n",
        "            \"tool_args\": func_args,\n",
        "            \"tool_result\": tool_result,\n",
        "            \"router_latency_ms\": route_result[\"latency_ms\"],\n",
        "            \"total_latency_ms\": total_latency,\n",
        "            \"used_llm\": False\n",
        "        }\n",
        "    \n",
        "    # Step 2: Use Gemma for conversation\n",
        "    llm_result = chat_with_gemma(user_message, history)\n",
        "    \n",
        "    total_latency = (time.time() - total_start) * 1000\n",
        "    return {\n",
        "        \"text\": llm_result[\"text\"],\n",
        "        \"tool_called\": None,\n",
        "        \"router_latency_ms\": route_result[\"latency_ms\"],\n",
        "        \"llm_latency_ms\": llm_result[\"latency_ms\"],\n",
        "        \"total_latency_ms\": total_latency,\n",
        "        \"used_llm\": True\n",
        "    }\n",
        "\n",
        "# Test complete pipeline\n",
        "print(\"Testing Complete Pipeline:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "pipeline_tests = [\n",
        "    \"Hello!\",\n",
        "    \"What times do you have available tomorrow?\",\n",
        "    \"I'd like to book an appointment for tomorrow at 2pm. My name is Sarah.\",\n",
        "    \"Thanks, can you tell me about your pricing?\",\n",
        "    \"I want to speak to a manager about my last visit\",\n",
        "]\n",
        "\n",
        "history = []\n",
        "latencies = []\n",
        "\n",
        "for msg in pipeline_tests:\n",
        "    result = voice_pipeline(msg, history)\n",
        "    print(f\"\\nUser: {msg}\")\n",
        "    print(f\"Response: {result['text']}\")\n",
        "    print(f\"Tool called: {result.get('tool_called', 'None')}\")\n",
        "    print(f\"Used LLM: {result['used_llm']}\")\n",
        "    print(f\"Router: {result['router_latency_ms']:.1f}ms | Total: {result['total_latency_ms']:.1f}ms\")\n",
        "    \n",
        "    latencies.append(result['total_latency_ms'])\n",
        "    history.append({\"role\": \"user\", \"content\": msg})\n",
        "    history.append({\"role\": \"assistant\", \"content\": result['text']})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Average latency: {sum(latencies)/len(latencies):.1f}ms\")\n",
        "print(f\"Min latency: {min(latencies):.1f}ms\")\n",
        "print(f\"Max latency: {max(latencies):.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Benchmark: Throughput Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Benchmark messages\n",
        "benchmark_messages = [\n",
        "    \"Hi\",\n",
        "    \"Hello there\",\n",
        "    \"I want to book an appointment\",\n",
        "    \"What times are available?\",\n",
        "    \"Do you have anything tomorrow?\",\n",
        "    \"Can I book for 2pm?\",\n",
        "    \"What are your hours?\",\n",
        "    \"How much does a haircut cost?\",\n",
        "    \"I need to reschedule\",\n",
        "    \"Thanks, bye!\",\n",
        "] * 5  # 50 messages\n",
        "\n",
        "random.shuffle(benchmark_messages)\n",
        "\n",
        "print(f\"Running benchmark with {len(benchmark_messages)} messages...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "router_latencies = []\n",
        "total_latencies = []\n",
        "llm_used_count = 0\n",
        "\n",
        "benchmark_start = time.time()\n",
        "\n",
        "for i, msg in enumerate(benchmark_messages):\n",
        "    result = voice_pipeline(msg)\n",
        "    router_latencies.append(result['router_latency_ms'])\n",
        "    total_latencies.append(result['total_latency_ms'])\n",
        "    if result['used_llm']:\n",
        "        llm_used_count += 1\n",
        "    \n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(benchmark_messages)}\")\n",
        "\n",
        "benchmark_time = time.time() - benchmark_start\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BENCHMARK RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total messages: {len(benchmark_messages)}\")\n",
        "print(f\"Total time: {benchmark_time:.1f}s\")\n",
        "print(f\"Throughput: {len(benchmark_messages)/benchmark_time:.1f} messages/sec\")\n",
        "print(f\"\\nRouter latency:\")\n",
        "print(f\"  Mean: {sum(router_latencies)/len(router_latencies):.1f}ms\")\n",
        "print(f\"  Min: {min(router_latencies):.1f}ms\")\n",
        "print(f\"  Max: {max(router_latencies):.1f}ms\")\n",
        "print(f\"\\nTotal latency:\")\n",
        "print(f\"  Mean: {sum(total_latencies)/len(total_latencies):.1f}ms\")\n",
        "print(f\"  Min: {min(total_latencies):.1f}ms\")\n",
        "print(f\"  Max: {max(total_latencies):.1f}ms\")\n",
        "print(f\"\\nLLM usage: {llm_used_count}/{len(benchmark_messages)} ({100*llm_used_count/len(benchmark_messages):.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optional: Qwen 32B for Complex Queries\n",
        "\n",
        "Uncomment to test Qwen 32B (requires more VRAM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to load Qwen 32B\n",
        "# Requires ~70GB VRAM for full precision, ~35GB with 4-bit quantization\n",
        "\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "\n",
        "# print(\"Loading Qwen3 32B...\")\n",
        "# qwen_model_id = \"Qwen/Qwen2.5-32B-Instruct\"  # Using Qwen 2.5 as Qwen 3 not available yet\n",
        "\n",
        "# qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_model_id)\n",
        "# qwen_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     qwen_model_id,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=\"auto\",\n",
        "#     load_in_4bit=True,  # Use 4-bit to fit in memory\n",
        "# )\n",
        "# print(f\"Qwen loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Memory Usage Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "print(\"GPU Memory Usage:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "print(f\"Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "# Free memory if needed\n",
        "# del fg_model, gemma_model\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Results:**\n",
        "- FunctionGemma router: ~X ms average latency\n",
        "- Direct tool calls: ~X ms total (no LLM needed)\n",
        "- LLM conversation: ~X ms total\n",
        "\n",
        "**Recommendations:**\n",
        "1. Use FunctionGemma 270M for routing (very fast)\n",
        "2. Use Gemma 2 9B for most conversations\n",
        "3. Reserve Qwen 32B for complex edge cases\n",
        "\n",
        "**Next Steps:**\n",
        "- Deploy FunctionGemma on Ollama for production\n",
        "- Use Groq API for Llama 3.1 8B (free tier)\n",
        "- Consider RunPod/Modal for Gemma 3 12B hosting"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}